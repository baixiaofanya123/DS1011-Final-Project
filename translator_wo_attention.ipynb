{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v14WqIj8VcmL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zhon in ./pytorch-gpu/nlp/lib/python3.6/site-packages (1.1.5)\r\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "! pip install zhon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Some part of this code is modified from NLP lab code'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNYNZsxFVcmT"
   },
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "UNK_token = 3\n",
    "\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    ''' \n",
    "    Class to build word vaocabulary\n",
    "    '''\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {\"UNK\":3}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\", 3: \"UNK\"}\n",
    "        self.n_words = 4  # Count PAD, SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "\n",
    "    def trim_sentences(self, min_count):\n",
    "        if self.trimmed: \n",
    "            return\n",
    "        self.trimmed = True\n",
    "        \n",
    "        words_keep = []\n",
    "        \n",
    "        for i, j in self.word2count.items():\n",
    "            if j >= min_count:\n",
    "                words_keep.append(i)\n",
    "\n",
    "        print('keep_words %s / %s = %.3f' % (\n",
    "            len(words_keep), len(self.word2index), len(words_keep) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "\n",
    "        self.word2index = {\"UNK\":3}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\", 3: \"UNK\"}\n",
    "        self.n_words = 4 \n",
    "        \n",
    "        for word in words_keep:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "4yDmmln9VcmW",
    "outputId": "c8af49f8-087a-4930-c498-2cfb387804f6"
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString_en(s):\n",
    "    '''\n",
    "    Normalize function for English and Vietnamese\n",
    "    '''\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"&apos\", r\"\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "from zhon.hanzi import punctuation\n",
    "def normalizeString_zh(s):\n",
    "    '''\n",
    "    Normalize function for Chinese\n",
    "    '''\n",
    "    \n",
    "    punc = '＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､\\u3000、〃〈〉《》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏﹑﹔·.'\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([。！？])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[0-9]\", r\" \", s)\n",
    "    s = re.sub(r\"[%s]+\" %punc, r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g0rOOp5oVcmY"
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    '''\n",
    "    function to read in data file\n",
    "    '''\n",
    "    \n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines1 = open(lang1, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    \n",
    "    lines2 = open(lang2, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [None] * len(lines1)\n",
    "    print(len(pairs))\n",
    "    for i in range(len(lines1)):\n",
    "        pairs[i] = [normalizeString_en(lines1[i]), normalizeString_en(lines2[i])]\n",
    "        \n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        #set object\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ratIMRaVcme"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 9999\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    '''\n",
    "    Filter maximum length\n",
    "    '''\n",
    "    \n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        if len(pair[0]) <= MAX_LENGTH \\\n",
    "            and len(pair[1]) <= MAX_LENGTH:\n",
    "                keep_pairs.append(pair)\n",
    "    return keep_pairs    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HDpgCNVEVcmg"
   },
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    \n",
    "    '''\n",
    "    Assemble function\n",
    "    '''\n",
    "    \n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    #print(input_lang.word2index)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "TIhWxJPBVcmk",
    "outputId": "d8a9a567-fa0a-4a97-a6c7-8dc6f5ea8b32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "133317\n",
      "Read 133317 sentence pairs\n",
      "Trimmed to 133317 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "train.tok.vi 14237\n",
      "train.tok.en 41271\n",
      "Reading lines...\n",
      "1553\n",
      "Read 1553 sentence pairs\n",
      "Trimmed to 1553 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "test.tok.vi 1071\n",
      "test.tok.en 3407\n",
      "['usda cho phep cac thuoc khang sinh cac hormon va cac thuoc tru sau trong nguon cung thuc pham cua ta va usda tra tien cho quang cao nay trong tap chi time .', 'the usda allows these antibiotics these hormones and these pesticides in our food supply and the usda paid for this ad in time magazine .']\n"
     ]
    }
   ],
   "source": [
    "train_input_lang, train_output_lang, train_pairs = prepareData('train.tok.en', 'train.tok.vi', True)\n",
    "test_input_lang, test_output_lang, test_pairs = prepareData('test.tok.en', 'test.tok.vi', True)\n",
    "\n",
    "print(random.choice(train_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "PuiYS0k1mGLf",
    "outputId": "7f0f6392-48cd-4cd4-a2b2-64f32ada6aac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "1268\n",
      "Read 1268 sentence pairs\n",
      "Trimmed to 1268 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "dev.tok.vi 1105\n",
      "dev.tok.en 3571\n"
     ]
    }
   ],
   "source": [
    "val_input_lang, val_output_lang, val_pairs = prepareData('dev.tok.en', 'dev.tok.vi', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "UzoqI4QQVcms",
    "outputId": "47040a7e-e248-4336-a5d5-b06b7812cd93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 7844 / 14234 = 0.551\n",
      "keep_words 27395 / 41268 = 0.664\n"
     ]
    }
   ],
   "source": [
    "# Trim the words based on the min word count\n",
    "MIN_COUNT = 2\n",
    "\n",
    "train_input_lang.trim_sentences(MIN_COUNT)\n",
    "train_output_lang.trim_sentences(MIN_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AammMPP1WTjN"
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    out = []\n",
    "    for word in sentence.split(' '):\n",
    "        if word in lang.word2index.keys():\n",
    "            out.append(lang.word2index[word])\n",
    "        else:\n",
    "            out.append(lang.word2index['UNK'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KMNoqdvZVcm3"
   },
   "outputs": [],
   "source": [
    "##dataloader\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "PAD_IDX = 0\n",
    "SOS_IDX = 1\n",
    "EOS_IDX = 2\n",
    "UNK_IDX = 3\n",
    "\n",
    "class MTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_lang, output_lang, pairs):\n",
    "        \"\"\"\n",
    "        @param candiate_list: list of candidate sentence\n",
    "        @param reference_list: list of reference sentence\n",
    "\n",
    "        \"\"\"\n",
    "        self.pairs = pairs\n",
    "        self.input_lang = input_lang\n",
    "        self.output_lang = output_lang\n",
    "\n",
    "        self.candidate_list = [indexesFromSentence(self.input_lang, pair[0]) for pair in pairs]\n",
    "        self.reference_list = [indexesFromSentence(self.output_lang, pair[1]) for pair in pairs]\n",
    "\n",
    "        assert (len(self.candidate_list) == len(self.reference_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        candidate_idx = self.candidate_list[key][:MAX_LENGTH]\n",
    "        reference_idx = self.reference_list[key][:MAX_LENGTH]\n",
    "        candidate_idx = [SOS_IDX] + candidate_idx + [EOS_IDX]\n",
    "        reference_idx = [SOS_IDX] + reference_idx + [EOS_IDX]\n",
    "\n",
    "        return [candidate_idx, len(candidate_idx), reference_idx, len(reference_idx)]\n",
    "    \n",
    "\n",
    "def MT_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    candidate_list = []\n",
    "    reference_list = []\n",
    "    candidate_length_list = []\n",
    "    reference_length_list = []\n",
    "    for datum in batch:\n",
    "        candidate_length_list.append(datum[1])\n",
    "        reference_length_list.append(datum[3])\n",
    "    # padding\n",
    "    MAX_LENGTH = [max(candidate_length_list), max(reference_length_list)]\n",
    "    for datum in batch:\n",
    "        padded_vec_1 = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_LENGTH[0]-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        candidate_list.append(padded_vec_1)\n",
    "        \n",
    "        padded_vec_2 = np.pad(np.array(datum[2]), \n",
    "                                pad_width=((0,MAX_LENGTH[1]-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        reference_list.append(padded_vec_2)\n",
    "    \n",
    "    sorted_order = np.argsort(candidate_length_list)[::-1]\n",
    "    candidate_list, candidate_length_list = np.array(candidate_list)[sorted_order], np.array(candidate_length_list)[sorted_order]\n",
    "    reference_list, reference_length_list = np.array(reference_list)[sorted_order], np.array(reference_length_list)[sorted_order]\n",
    "    \n",
    "    return [torch.from_numpy(np.array(candidate_list)), torch.LongTensor(candidate_length_list), \n",
    "            torch.from_numpy(np.array(reference_list)), torch.LongTensor(reference_length_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "764PKl2nVcm5"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset = MTDataset(train_input_lang, train_output_lang, train_pairs)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=MT_collate_func,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory = True)\n",
    "\n",
    "val_dataset = MTDataset(train_input_lang, train_output_lang, val_pairs)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=MT_collate_func,\n",
    "                                           shuffle=False,\n",
    "                                           pin_memory = True)\n",
    "\n",
    "test_dataset = MTDataset(train_input_lang, train_output_lang, test_pairs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=MT_collate_func,\n",
    "                                           shuffle=False,\n",
    "                                           pin_memory = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvtvSAYFVcnP"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    '''\n",
    "    One layer bidurectional GRU model\n",
    "    '''\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, num_layers=num_layers, bidirectional=True)\n",
    "\n",
    "    def forward(self, input, input_length, hidden=None):\n",
    "         \n",
    "\n",
    "        batch_size = input.size(0)\n",
    "\n",
    "        embedded = self.embedding(input.transpose(0,1))\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_length) \n",
    "\n",
    "        output, hidden = self.gru(packed, hidden)\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(output)\n",
    "        output = output[:, :, :self.hidden_size] + output[:, : ,self.hidden_size:]\n",
    "        hidden = hidden[:self.num_layers, :, :] + hidden[self.num_layers:,:,:]\n",
    "        \n",
    "        return output, hidden\n",
    "     \n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers*2, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ycwZ2764VcnT"
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, n_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "\n",
    "        \n",
    "        input = input.view(1,-1)\n",
    "        _, batch_size = input.size()\n",
    "\n",
    "        \n",
    "        output = self.embedding(input).view(1, batch_size, self.hidden_size)\n",
    "\n",
    "        output = F.relu(output)\n",
    "\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self,batch_size):\n",
    "        return Variable(torch.zeros(1, batch_size, self.hidden_size, device = device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5u75NVYTVcnV"
   },
   "outputs": [],
   "source": [
    "###minibatch\n",
    "teacher_forcing_ratio = 1\n",
    "\n",
    "def train(input_tensor, target_tensor, input_length, output_length, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    #encoder_hidden = encoder.initHidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    batch_size= input_tensor.size(0)\n",
    "    target_length = target_tensor.size(1)\n",
    "    \n",
    "    input_tensor = input_tensor.cuda()\n",
    "    target_tensor = target_tensor.cuda()\n",
    "    \n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor, input_length, None)\n",
    "    \n",
    "    loss = 0\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_IDX]*batch_size], device=device)\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        \n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            #import pdb; pdb.set_trace()\n",
    "            loss += criterion(decoder_output, target_tensor[:,di])\n",
    "            decoder_input = target_tensor[:,di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[:,di])\n",
    "\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5CTdFa8rVcnZ"
   },
   "outputs": [],
   "source": [
    "###minibatch\n",
    "def trainEpochs(encoder, decoder, n_epochs, print_every, plot_every, learning_rate):\n",
    "    start = time.time()\n",
    "\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for i, (candidate, length_1, reference, length_2) in enumerate(train_loader):\n",
    " \n",
    "            loss = train(candidate, reference, length_1, length_2, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            print_loss_total += loss\n",
    "   \n",
    "            plot_loss_total += loss\n",
    "\n",
    "            if i > 0 and i % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                #print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters), iter, iter / n_iters * 100, print_loss_avg))\n",
    "                print('Time: {}, Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}'.format(timeSince(start, i+1 / len(train_loader)), \n",
    "                    epoch+1, n_epochs, i+1, len(train_loader), print_loss_avg))\n",
    "            if i >0 and i % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "                \n",
    "        print_loss_total = 0\n",
    "        plot_loss_total = 0\n",
    "        \n",
    "        s, output_words, true_words = evaluateRandomly(encoder1, decoder1, val_loader)\n",
    "        print('-------vali score---------')\n",
    "        print('score {}'.format(s))\n",
    "        print('---------Save trained model--------')\n",
    "        torch.save(encoder1.state_dict(), \"encoder_wo_attn{}.pth\".format(epoch+1))\n",
    "        torch.save(decoder1.state_dict(), \"decoder_wo_attn{}.pth\".format(epoch+1))\n",
    "        \n",
    "        \n",
    "    showPlot(plot_losses)\n",
    "    return plot_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B5ErKHbuYWvp"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OGELiHahYTRK"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jfeurgKYVcnb"
   },
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "n_layers = 1\n",
    "dropout = 0.1\n",
    "batch_size = 32\n",
    "\n",
    "# Configure training/optimization\n",
    "#clip = 50.0\n",
    "teacher_forcing_ratio = 1\n",
    "learning_rate = 0.0005\n",
    "#decoder_learning_ratio = 5.0\n",
    "n_epochs = 10\n",
    "epoch = 0\n",
    "plot_every = 20\n",
    "print_every = 500\n",
    "evaluate_every = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BMYsVJKYlUjo"
   },
   "outputs": [],
   "source": [
    "encoder1 = EncoderRNN(train_input_lang.n_words, hidden_size, num_layers=1).to(device)\n",
    "decoder1 = DecoderRNN(train_output_lang.n_words, hidden_size, n_layers = 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I5zHjq5c8PEO"
   },
   "outputs": [],
   "source": [
    "encoder1.load_state_dict(torch.load(\"encoder_wo_attn1.pth\"))\n",
    "decoder1.load_state_dict(torch.load(\"decoder_wo_attn1.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 887
    },
    "colab_type": "code",
    "id": "nUD-Bzi8Vcne",
    "outputId": "f23e0972-58e1-4de7-d8d7-2b3834168308"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2m 14s (- -3m 45s), Epoch: [1/10], Step: [501/4167], Train Loss: 1.3796693702717406\n",
      "Time: 4m 27s (- -5m 32s), Epoch: [1/10], Step: [1001/4167], Train Loss: 1.358652635617888\n",
      "Time: 6m 40s (- -7m 20s), Epoch: [1/10], Step: [1501/4167], Train Loss: 1.3595095971989277\n",
      "Time: 8m 51s (- -9m 8s), Epoch: [1/10], Step: [2001/4167], Train Loss: 1.3399361855020766\n",
      "Time: 11m 2s (- -12m 57s), Epoch: [1/10], Step: [2501/4167], Train Loss: 1.316713573988913\n",
      "Time: 13m 13s (- -14m 46s), Epoch: [1/10], Step: [3001/4167], Train Loss: 1.3152373194315716\n",
      "Time: 15m 28s (- -16m 32s), Epoch: [1/10], Step: [3501/4167], Train Loss: 1.3052257556206308\n",
      "Time: 17m 44s (- -18m 16s), Epoch: [1/10], Step: [4001/4167], Train Loss: 1.2837309225181024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS i didn t have to go from the million people who have been a million dollars but i was going to be a million people who have been a million dollars but i was going to be a lot of people who are not\n",
      "SOS i had no idea what life was going to be like as a north korean refugee but i soon learned it s not only extremely difficult it s also very dangerous since north korean refugees are considered in china as illegal migrants .\n",
      "-------vali score---------\n",
      "score BLEU(score=7.246089940093556, counts=[10273, 2971, 1069, 427], totals=[28602, 27334, 26066, 24798], precisions=[35.917068736451995, 10.869247091534353, 4.101127906084555, 1.7219130575046375], bp=1.0, sys_len=28602, ref_len=26162)\n",
      "---------Save trained model--------\n",
      "Time: 21m 0s (- -21m 1s), Epoch: [2/10], Step: [501/4167], Train Loss: 1.1675726089586538\n",
      "Time: 23m 15s (- -24m 45s), Epoch: [2/10], Step: [1001/4167], Train Loss: 1.152335923180405\n",
      "Time: 25m 27s (- -26m 33s), Epoch: [2/10], Step: [1501/4167], Train Loss: 1.1444833154973848\n",
      "Time: 27m 40s (- -28m 20s), Epoch: [2/10], Step: [2001/4167], Train Loss: 1.1597522035316232\n",
      "Time: 29m 56s (- -30m 4s), Epoch: [2/10], Step: [2501/4167], Train Loss: 1.1664035378856101\n",
      "Time: 32m 4s (- -33m 55s), Epoch: [2/10], Step: [3001/4167], Train Loss: 1.1857493117554698\n",
      "Time: 34m 17s (- -35m 42s), Epoch: [2/10], Step: [3501/4167], Train Loss: 1.1601467805470582\n",
      "Time: 36m 30s (- -37m 30s), Epoch: [2/10], Step: [4001/4167], Train Loss: 1.1596225098543103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS i didn t even have to tell the million deaths but i was going to have a million dollars in the united states but it was the same thing that the united states is going to be the richest of the north africa because\n",
      "SOS i had no idea what life was going to be like as a north korean refugee but i soon learned it s not only extremely difficult it s also very dangerous since north korean refugees are considered in china as illegal migrants .\n",
      "-------vali score---------\n",
      "score BLEU(score=7.731039195681839, counts=[10476, 3076, 1136, 461], totals=[28157, 26889, 25621, 24353], precisions=[37.20566821749476, 11.439622150321693, 4.433862846883416, 1.892990596641071], bp=1.0, sys_len=28157, ref_len=26162)\n",
      "---------Save trained model--------\n",
      "Time: 39m 53s (- -40m 11s), Epoch: [3/10], Step: [501/4167], Train Loss: 1.003379777745962\n",
      "Time: 42m 4s (- -43m 57s), Epoch: [3/10], Step: [1001/4167], Train Loss: 1.0237047343374581\n",
      "Time: 44m 18s (- -45m 42s), Epoch: [3/10], Step: [1501/4167], Train Loss: 1.0411243920783115\n",
      "Time: 46m 36s (- -47m 25s), Epoch: [3/10], Step: [2001/4167], Train Loss: 1.0277162704565197\n",
      "Time: 48m 47s (- -49m 13s), Epoch: [3/10], Step: [2501/4167], Train Loss: 1.057476836948593\n",
      "Time: 51m 3s (- -52m 57s), Epoch: [3/10], Step: [3001/4167], Train Loss: 1.059099270321836\n",
      "Time: 53m 14s (- -54m 46s), Epoch: [3/10], Step: [3501/4167], Train Loss: 1.0621750072566056\n",
      "Time: 55m 24s (- -56m 35s), Epoch: [3/10], Step: [4001/4167], Train Loss: 1.0636336018442463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS i didn t even know about million people but it was a million people who had died of poverty but i was going to be a victim of the people who had died of poverty but the death toll was not only about a\n",
      "SOS i had no idea what life was going to be like as a north korean refugee but i soon learned it s not only extremely difficult it s also very dangerous since north korean refugees are considered in china as illegal migrants .\n",
      "-------vali score---------\n",
      "score BLEU(score=8.457942163945331, counts=[10541, 3121, 1183, 498], totals=[26750, 25482, 24214, 22946], precisions=[39.40560747663552, 12.247861235381839, 4.8856033699512675, 2.1703129085679422], bp=1.0, sys_len=26750, ref_len=26162)\n",
      "---------Save trained model--------\n",
      "Time: 58m 49s (- -59m 17s), Epoch: [4/10], Step: [501/4167], Train Loss: 0.9275924887882191\n",
      "Time: 61m 6s (- -62m 57s), Epoch: [4/10], Step: [1001/4167], Train Loss: 0.9277849679770874\n",
      "Time: 63m 22s (- -64m 40s), Epoch: [4/10], Step: [1501/4167], Train Loss: 0.929974954812341\n",
      "Time: 65m 34s (- -66m 27s), Epoch: [4/10], Step: [2001/4167], Train Loss: 0.957403276287424\n",
      "Time: 67m 44s (- -68m 16s), Epoch: [4/10], Step: [2501/4167], Train Loss: 0.96427725451167\n",
      "Time: 69m 59s (- -70m 1s), Epoch: [4/10], Step: [3001/4167], Train Loss: 0.9571913594015232\n",
      "Time: 72m 11s (- -73m 49s), Epoch: [4/10], Step: [3501/4167], Train Loss: 0.9675819861756741\n",
      "Time: 74m 27s (- -75m 33s), Epoch: [4/10], Step: [4001/4167], Train Loss: 0.9809940661944392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS i didn t even hear that one million people but i was not afraid of the refugees of refugees but it was a very difficult thing to be in the middle of the refugees but the idea of the north atlantic ocean is going\n",
      "SOS i had no idea what life was going to be like as a north korean refugee but i soon learned it s not only extremely difficult it s also very dangerous since north korean refugees are considered in china as illegal migrants .\n",
      "-------vali score---------\n",
      "score BLEU(score=8.372010628714495, counts=[10702, 3200, 1226, 519], totals=[27748, 26480, 25212, 23944], precisions=[38.56854548075537, 12.084592145015106, 4.862763763287323, 2.1675576344804544], bp=1.0, sys_len=27748, ref_len=26162)\n",
      "---------Save trained model--------\n",
      "Time: 77m 43s (- -78m 26s), Epoch: [5/10], Step: [501/4167], Train Loss: 0.8423107770262616\n",
      "Time: 79m 56s (- -80m 8s), Epoch: [5/10], Step: [1001/4167], Train Loss: 0.8534909424377295\n",
      "Time: 82m 7s (- -83m 55s), Epoch: [5/10], Step: [1501/4167], Train Loss: 0.870699192811415\n",
      "Time: 84m 27s (- -85m 34s), Epoch: [5/10], Step: [2001/4167], Train Loss: 0.8719172402109229\n",
      "Time: 86m 40s (- -87m 21s), Epoch: [5/10], Step: [2501/4167], Train Loss: 0.8920686489821188\n",
      "Time: 88m 53s (- -89m 7s), Epoch: [5/10], Step: [3001/4167], Train Loss: 0.9085878906280289\n",
      "Time: 91m 10s (- -92m 50s), Epoch: [5/10], Step: [3501/4167], Train Loss: 0.8954797580050288\n",
      "Time: 93m 19s (- -94m 41s), Epoch: [5/10], Step: [4001/4167], Train Loss: 0.9295948864800748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS i didn t even hear about billion dollars but it s not about billion people in the s but i think that there are a million people in africa but it s a difficult to fight for a crime of death or even refugees\n",
      "SOS i had no idea what life was going to be like as a north korean refugee but i soon learned it s not only extremely difficult it s also very dangerous since north korean refugees are considered in china as illegal migrants .\n",
      "-------vali score---------\n",
      "score BLEU(score=8.001474039702265, counts=[10898, 3171, 1185, 505], totals=[28591, 27323, 26055, 24787], precisions=[38.116889930397676, 11.605606997767449, 4.5480713874496255, 2.037358292653407], bp=1.0, sys_len=28591, ref_len=26162)\n",
      "---------Save trained model--------\n",
      "Time: 96m 51s (- -97m 19s), Epoch: [6/10], Step: [501/4167], Train Loss: 0.7858768178369475\n",
      "Time: 99m 10s (- -100m 55s), Epoch: [6/10], Step: [1001/4167], Train Loss: 0.7767923611180135\n",
      "Time: 101m 20s (- -102m 43s), Epoch: [6/10], Step: [1501/4167], Train Loss: 0.8193620640141726\n",
      "Time: 103m 33s (- -104m 29s), Epoch: [6/10], Step: [2001/4167], Train Loss: 0.8272413653221952\n",
      "Time: 105m 49s (- -106m 13s), Epoch: [6/10], Step: [2501/4167], Train Loss: 0.817030972247066\n",
      "Time: 108m 3s (- -109m 58s), Epoch: [6/10], Step: [3001/4167], Train Loss: 0.8372471668424921\n",
      "Time: 110m 17s (- -111m 44s), Epoch: [6/10], Step: [3501/4167], Train Loss: 0.8467883286435429\n",
      "Time: 112m 31s (- -113m 30s), Epoch: [6/10], Step: [4001/4167], Train Loss: 0.8596711800490027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS i didn t have a million refugees that would have been visited but not to mention north africa s most difficult to reach a refugee but the death of the pyramid of the refugees would be the same thing but the north pole was\n",
      "SOS i had no idea what life was going to be like as a north korean refugee but i soon learned it s not only extremely difficult it s also very dangerous since north korean refugees are considered in china as illegal migrants .\n",
      "-------vali score---------\n",
      "score BLEU(score=7.91853967926915, counts=[10789, 3142, 1160, 489], totals=[28385, 27117, 25849, 24581], precisions=[38.00951206623216, 11.586827451414242, 4.487601067739565, 1.9893413612139457], bp=1.0, sys_len=28385, ref_len=26162)\n",
      "---------Save trained model--------\n",
      "Time: 116m 2s (- -116m 11s), Epoch: [7/10], Step: [501/4167], Train Loss: 0.7262077723101394\n",
      "Time: 118m 18s (- -119m 48s), Epoch: [7/10], Step: [1001/4167], Train Loss: 0.7428285961627228\n",
      "Time: 120m 31s (- -121m 33s), Epoch: [7/10], Step: [1501/4167], Train Loss: 0.7626837660523591\n",
      "Time: 122m 45s (- -123m 18s), Epoch: [7/10], Step: [2001/4167], Train Loss: 0.7677475801381975\n",
      "Time: 124m 57s (- -125m 5s), Epoch: [7/10], Step: [2501/4167], Train Loss: 0.7997240159447213\n",
      "Time: 127m 10s (- -128m 51s), Epoch: [7/10], Step: [3001/4167], Train Loss: 0.8055998457577185\n",
      "Time: 129m 24s (- -130m 37s), Epoch: [7/10], Step: [3501/4167], Train Loss: 0.8101014662194751\n",
      "Time: 131m 37s (- -132m 24s), Epoch: [7/10], Step: [4001/4167], Train Loss: 0.8188201788635291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS i didn t have a situation in the north of the refugees but i was devastated for a million dollars a year but a refugee child survival was dying in the north africa .\n",
      "SOS i had no idea what life was going to be like as a north korean refugee but i soon learned it s not only extremely difficult it s also very dangerous since north korean refugees are considered in china as illegal migrants .\n",
      "-------vali score---------\n",
      "score BLEU(score=8.235938408184898, counts=[10639, 3115, 1166, 505], totals=[27461, 26193, 24925, 23657], precisions=[38.74221623393176, 11.892490360019853, 4.6780341023069205, 2.1346747262966566], bp=1.0, sys_len=27461, ref_len=26162)\n",
      "---------Save trained model--------\n",
      "Time: 135m 0s (- -135m 16s), Epoch: [8/10], Step: [501/4167], Train Loss: 0.6826451477919455\n",
      "Time: 137m 10s (- -138m 57s), Epoch: [8/10], Step: [1001/4167], Train Loss: 0.7034927001260772\n",
      "Time: 139m 22s (- -140m 42s), Epoch: [8/10], Step: [1501/4167], Train Loss: 0.724800356084746\n",
      "Time: 141m 35s (- -142m 28s), Epoch: [8/10], Step: [2001/4167], Train Loss: 0.7375669886729849\n",
      "Time: 143m 52s (- -144m 11s), Epoch: [8/10], Step: [2501/4167], Train Loss: 0.7470539430393749\n",
      "Time: 146m 4s (- -147m 58s), Epoch: [8/10], Step: [3001/4167], Train Loss: 0.7565100086766724\n",
      "Time: 148m 15s (- -149m 46s), Epoch: [8/10], Step: [3501/4167], Train Loss: 0.7821190587779027\n",
      "Time: 150m 27s (- -151m 35s), Epoch: [8/10], Step: [4001/4167], Train Loss: 0.7843330748288677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS i didn t have a million refugees in the north of new jersey but i was going to die but if i was a criminal prosecutor with the refugees of refugees who had been raped from the north atlantic ocean north africa .\n",
      "SOS i had no idea what life was going to be like as a north korean refugee but i soon learned it s not only extremely difficult it s also very dangerous since north korean refugees are considered in china as illegal migrants .\n",
      "-------vali score---------\n",
      "score BLEU(score=7.837115981570091, counts=[10757, 3058, 1122, 478], totals=[28091, 26823, 25555, 24287], precisions=[38.293403581218186, 11.400663609588786, 4.39053022891802, 1.9681310989418208], bp=1.0, sys_len=28091, ref_len=26162)\n",
      "---------Save trained model--------\n",
      "Time: 154m 0s (- -154m 17s), Epoch: [9/10], Step: [501/4167], Train Loss: 0.6481255815513448\n",
      "Time: 156m 13s (- -157m 55s), Epoch: [9/10], Step: [1001/4167], Train Loss: 0.6742613934886457\n",
      "Time: 158m 24s (- -159m 42s), Epoch: [9/10], Step: [1501/4167], Train Loss: 0.7017987462007154\n",
      "Time: 160m 38s (- -161m 26s), Epoch: [9/10], Step: [2001/4167], Train Loss: 0.7026719074940817\n",
      "Time: 162m 49s (- -163m 14s), Epoch: [9/10], Step: [2501/4167], Train Loss: 0.7156494132690523\n",
      "Time: 165m 2s (- -165m 0s), Epoch: [9/10], Step: [3001/4167], Train Loss: 0.7324637982310207\n",
      "Time: 167m 14s (- -168m 47s), Epoch: [9/10], Step: [3501/4167], Train Loss: 0.7360990213275765\n",
      "Time: 169m 29s (- -170m 32s), Epoch: [9/10], Step: [4001/4167], Train Loss: 0.740954320902026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS i couldn t talk about million dollars but it s about a decade but it s difficult to reach the north of crime but it s also about the west bank as the master s good thing to do .\n",
      "SOS i had no idea what life was going to be like as a north korean refugee but i soon learned it s not only extremely difficult it s also very dangerous since north korean refugees are considered in china as illegal migrants .\n",
      "-------vali score---------\n",
      "score BLEU(score=7.810051974081473, counts=[10632, 3003, 1116, 468], totals=[27815, 26547, 25279, 24011], precisions=[38.22397986697825, 11.312012656797378, 4.414731595395388, 1.949106659447753], bp=1.0, sys_len=27815, ref_len=26162)\n",
      "---------Save trained model--------\n",
      "Time: 172m 58s (- -173m 22s), Epoch: [10/10], Step: [501/4167], Train Loss: 0.6327316100957352\n",
      "Time: 175m 10s (- -175m 0s), Epoch: [10/10], Step: [1001/4167], Train Loss: 0.6538031099728917\n",
      "Time: 177m 18s (- -178m 48s), Epoch: [10/10], Step: [1501/4167], Train Loss: 0.6698027216332805\n",
      "Time: 179m 28s (- -180m 36s), Epoch: [10/10], Step: [2001/4167], Train Loss: 0.6828364630178136\n",
      "Time: 181m 45s (- -182m 18s), Epoch: [10/10], Step: [2501/4167], Train Loss: 0.6931041100452832\n",
      "Time: 184m 1s (- -184m 2s), Epoch: [10/10], Step: [3001/4167], Train Loss: 0.6823384973455658\n",
      "Time: 186m 16s (- -187m 46s), Epoch: [10/10], Step: [3501/4167], Train Loss: 0.7083924501489053\n",
      "Time: 188m 32s (- -189m 30s), Epoch: [10/10], Step: [4001/4167], Train Loss: 0.7171810093802959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS i didn t tell me that when million refugees were coming from a refugee camp or a refugee camp but i was also a north carolina who died in the refugee camps but the north pole was about a terrible crime .\n",
      "SOS i had no idea what life was going to be like as a north korean refugee but i soon learned it s not only extremely difficult it s also very dangerous since north korean refugees are considered in china as illegal migrants .\n",
      "-------vali score---------\n",
      "score BLEU(score=7.853069259602653, counts=[10690, 3034, 1127, 472], totals=[27893, 26625, 25357, 24089], precisions=[38.32502778474886, 11.395305164319248, 4.444532081870884, 1.9594005562704968], bp=1.0, sys_len=27893, ref_len=26162)\n",
      "---------Save trained model--------\n"
     ]
    }
   ],
   "source": [
    "####Start training!\n",
    "#hidden_size = 300\n",
    "#learning_rate = 0.0002\n",
    "#number of epoch = 15\n",
    "#language = Vietnamese\n",
    "#n_epochs = 5\n",
    "train_loss = trainEpochs(encoder1, decoder1, n_epochs, print_every, plot_every, learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2njg6Tb965OF"
   },
   "outputs": [],
   "source": [
    "torch.save(encoder1.state_dict(), \"encoder_wo_attn_vi_300_0.0005.pth\")\n",
    "torch.save(decoder1.state_dict(), \"decoder_wo_attn_vi_300_0.0005.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r_PyezOQdVdn"
   },
   "outputs": [],
   "source": [
    "encoder1.load_state_dict(torch.load(\"encoder_wo_attn6.pth\"))\n",
    "decoder1.load_state_dict(torch.load(\"decoder_wo_attn6.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YyaGupwhx_rW"
   },
   "outputs": [],
   "source": [
    "def mapback(reference):\n",
    "\n",
    "    words = []\n",
    "    \n",
    "    for i in range(reference.size(0)):\n",
    "        line = []\n",
    "        for j in range(reference.size(1)):\n",
    "            \n",
    "            if int(reference[i,j].item()) == 1:\n",
    "                pass\n",
    "            \n",
    "            if int(reference[i,j].item()) == 2:\n",
    "                break\n",
    "            else:\n",
    "                line.append(train_output_lang.index2word[int(reference[i,j].item())])\n",
    "        \n",
    "        line.remove('SOS')\n",
    "        \n",
    "        line = ' '.join(line)\n",
    "        words.append(line)\n",
    "      \n",
    "    return words\n",
    "  \n",
    "  \n",
    "def evaluate_wo1(encoder, decoder, candidate, length_1, reference, length_2, max_length):\n",
    "  \n",
    "    with torch.no_grad():\n",
    "\n",
    "        batch_size = candidate.size(0)\n",
    "        candidate =  candidate.cuda()\n",
    "        reference = reference.cuda()\n",
    "        encoder_ouputs, encoder_hidden = encoder(candidate, length_1, None)\n",
    "        decoder_input = torch.tensor([[SOS_token]*batch_size], device = device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        #print(encoder_hidden.size())\n",
    "        batch_size = candidate.size(0)\n",
    "\n",
    "        decoded_words = torch.ones([batch_size, max_length])\n",
    "\n",
    "        for di in range(max_length):\n",
    "\n",
    "            try:\n",
    "\n",
    "                decoder_output, decoder_hidden = decoder(\n",
    "                    decoder_input, decoder_hidden)\n",
    "            except:\n",
    "\n",
    "                import pdb; pdb.set_trace()\n",
    "\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "\n",
    "            decoded_words[:,di] = topi.squeeze()\n",
    "\n",
    "\n",
    "\n",
    "            decoder_input = topi.squeeze().detach().cuda()\n",
    "            words = mapback(decoded_words) \n",
    "\n",
    "\n",
    "    return words\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RSw_zphPyCTI"
   },
   "outputs": [],
   "source": [
    "#! pip install sacrebleu\n",
    "import sacrebleu\n",
    "sacrebleu.corpus_bleu\n",
    "def evaluateRandomly(encoder1, decoder1, loader):\n",
    "  \n",
    "    score = 0\n",
    "    output_words = []\n",
    "    true_words = []\n",
    "    for i, (candidate, length_1, reference, length_2) in enumerate(loader):\n",
    "\n",
    "            max_length = max(length_2).item()\n",
    "            output_words += evaluate_wo1(encoder1, decoder1, candidate, length_1, reference, length_2,max_length)\n",
    "            true_words += mapback(reference)\n",
    "        \n",
    "    score = sacrebleu.corpus_bleu(output_words,[true_words])\n",
    "    \n",
    "    print(output_words[0])\n",
    "    print(true_words[0])\n",
    "        \n",
    "        \n",
    "    return (score, output_words, true_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "GyTjDvVZyHIm",
    "outputId": "d785a543-4080-4164-f402-053b71072534"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how can i make her life for the world s first child with the world s bowery life span of her hometown that her mother s been told her quot how long did not have a conversation with her life on her hair and her mom quot dolly quot and she said quot mom s born with her life on her skirt and her quot hometown of baby ? quot \n",
      "how can i speak in minutes about the bonds of women over three generations about how the astonishing strength of those bonds took hold in the life of a four year old girl huddled with her young sister her mother and her grandmother for five days and nights in a small boat in the china sea more than years ago bonds that took hold in the life of that small girl and never let go that small girl now living in san francisco and speaking to you today ?\n"
     ]
    }
   ],
   "source": [
    "s, output_words, true_words = evaluateRandomly(encoder1, decoder1, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "zcsn1mwA7KAe",
    "outputId": "502dbb1a-2746-4b66-fad1-9f6bfb510c41"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BLEU(score=7.204095684103843, counts=[10334, 2779, 988, 404], totals=[27928, 26375, 24822, 23276], precisions=[37.0022916069894, 10.53649289099526, 3.980340020949158, 1.7356934181130779], bp=1.0, sys_len=27928, ref_len=26525)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "5E5Rl6iyMxo5",
    "outputId": "7ab47569-bba2-4124-dc23-4898c1eae64f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= how can i speak in minutes about the bonds of women over three generations about how the astonishing strength of those bonds took hold in the life of a four year old girl huddled with her young sister her mother and her grandmother for five days and nights in a small boat in the china sea more than years ago bonds that took hold in the life of that small girl and never let go that small girl now living in san francisco and speaking to you today ?\n",
      "> how can i make her life for the world s first child with the world s bowery life span of her hometown that her mother s been told her quot how long did not have a conversation with her life on her hair and her mom quot dolly quot and she said quot mom s born with her life on her skirt and her quot hometown of baby ? quot \n",
      "= but i do remember the lights on the oil rig off the UNK coast and the young man who collapsed and died the journey s end too much for him and the first apple i tasted given to me by the men on the rig .\n",
      "> but i remember my brother as a result of a small act of exchange with my brother and my brother after my brother was the last time that was the first time that landed on my checklist and the result of my life was born .\n",
      "= i don t remember the pirates who came many times but were UNK by the UNK of the men on our boat or the engine dying and failing to start for six hours .\n",
      "> i didn t worry about that man didn t want to fly away from the air conditioning any longer without any of the people who had been killed with the flight or the flight of the war on the boat and the captain was killed .\n",
      "= my duty was not to allow it to have been in vain and my lesson was to learn that yes history tried to crush us but we endured .\n",
      "> my journey is that my journey and my wife and my life was not the result of the birth of history that we had learned about in the past years .\n",
      "= my first memories are from the boat the steady beat of the engine the bow dipping into each wave the vast and empty horizon .\n",
      "> my first birthday i remember the birth of the birth of the birth of the birth of the birth of the birth of the birth of the birth of the birth of the birth of the birth of the birth of the captain was born in the middle of the night .\n",
      "= he is a poet a playwright a man whose whole life had been balanced on the single hope of his country s unity and freedom .\n",
      "> he was a palestinian a palestinian state of a writer that was a serial killer in the country that would write the whole thing .\n",
      "= unlike the settled middle class suburbs whose existence i was oblivious of there was no sense of entitlement in UNK .\n",
      "> not unlike the negative part of the chicken that i m not in the center of the gulf of the pyramid that have the deficit to exist .\n",
      "= my mother UNK was when her father died already in an arranged marriage already with two small girls .\n",
      "> my mom was married when she was a little girl she was married by a young girl who had her mother was a sister .\n",
      "= so after a four year saga that defies fiction a boat slipped out to sea disguised as a fishing vessel .\n",
      "> so after the year of the year the hirshhorn was covered in the hull of the boat which was a billboard .\n",
      "= for her life had distilled itself into one task the escape of her family and a new life in australia .\n",
      "> with our mother had a birthday to have our defenses that most of our lives were forced to survive and to stay alive .\n",
      "= and the next piece of the jigsaw is about four women across three generations shaping a new life together .\n",
      "> and the next picture of the puzzle is the world that s the next generation that everyone sees each generation is the same .\n",
      "= imagine him as the communists enter UNK confronting the fact that his life had been a complete waste .\n",
      "> imagine a man who introduced the man of the future of the money that failed to be born .\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(output_words)):\n",
    "    print('= {}'.format(true_words[i]))\n",
    "    print('> {}'.format(output_words[i]))\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "KVW4j6reO6lB",
    "outputId": "dc897e14-e792-46aa-b95b-99723e98a57b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "  Downloading https://files.pythonhosted.org/packages/37/51/bffea2b666d59d77be0413d35220022040a1f308c39009e5b023bc4eb8ab/sacrebleu-1.2.12.tar.gz\n",
      "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (3.6.6)\n",
      "Building wheels for collected packages: sacrebleu\n",
      "  Running setup.py bdist_wheel for sacrebleu ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ea/0a/7d/ddcbdcd15a04b72de1b3f78e7e754aab415aff81c423376385\n",
      "Successfully built sacrebleu\n",
      "Installing collected packages: sacrebleu\n",
      "Successfully installed sacrebleu-1.2.12\n"
     ]
    }
   ],
   "source": [
    "train_input_lang_zh, train_output_lang_zh, train_pairs_zh = prepareData('train.tok.en', 'train.tok.zh', True)\n",
    "test_input_lang_zh, test_output_lang_zh, test_pairs_zh = prepareData('test.tok.en', 'test.tok.zh', True)\n",
    "\n",
    "print(random.choice(train_pairs))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "finalproject_v2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
